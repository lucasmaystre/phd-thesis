%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:conclusion}

In this work, we demonstrate that active learning can substantively speed up the task of learning a ranking from noisy comparisons gains---both in theory and in practice.
With the advent of large-scale crowdsourced ranking surveys, exemplified by GIFGIF and wiki surveys \citep{salganik2015wiki}, there is a clear need for practical AL strategies.
However, existing methods are complex and computationally expensive to operate even for a reasonable number of items (a few thousands).
We show that a deceptively simple idea---repeatedly sorting the items---is able to bring in all the benefits of active learning, is trivial to implement, and is computationally no more expensive that random sampling.
Therefore, we believe that our method can be broadly useful for machine-learning practitioners interested in ranking problems.

\paragraph{Acknowledgments.}
We thank Holly Cogliati-Bauereis, Ksenia Konyushkova, Brunella Spinelli and anonymous reviewers for careful proofreading and helpful comments.
