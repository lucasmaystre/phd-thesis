%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extensions and Proofs}  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{app:extensions}

In this section, we start by generalizing the network choice model to account for edge weights.
Then, we present formal proofs for
\begin{enuminline}
\item the (minimal) sufficiency of marginal counts and
\item the well-posedness of MAP inference
\end{enuminline}
in the generalized weighted network choice model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization of the Model}

Let $G = (V, E)$ be a weighted, directed graph with edge weights $w_{ij} > 0$ for all $(i, j) \in E$.
\citet{kumar2015inverting} propose the following generalization of Luce's choice model.
Given a parameter vector $\bm{\lambda} \in \mathbf{R}_{>0}^n$, they define the choice probabilities as
\begin{align}
\label{eq:wsinglelik}
p_{ij} = \frac{w_{ij} \lambda_j}{\sum_{k \in N^+_i} w_{ik} \lambda_k}, \quad j \in N^+_i.
\end{align}
We refer to this model as the \emph{weighted network choice model}.
Intuitively, the strength of each alternative is weighted by the corresponding edge's weight;
Luce's original choice model is obtained by setting $w_{ij} = \text{constant}$.
In this general model, the log-likelihood becomes
\begin{align}
\ell(\bm{\lambda} ; \mathcal{D})
    &= \sum_{(i,j) \in E} c_{ij} \bigg[ \log w_{ij} \lambda_j - \log \sum_{k \in N^+_i} w_{ik} \lambda_k \bigg] \nonumber \\
    &= \sum_{(i,j) \in E} c_{ij} \bigg[ \log \lambda_j - \log \sum_{k \in N^+_i} w_{ik} \lambda_k \bigg] \nonumber
       + \sum_{(i,j) \in E} c_{ij} \log w_{ij}, \nonumber \\
    &= \sum_{i = 1}^n \bigg[ c^-_i \log \lambda_i - c^+_i \log\!\sum_{k \in N^+_i}\!w_{ik} \lambda_k \bigg] + \kappa_1, \label{eq:wloglik}
\end{align}
where $c^-_i = \sum_{j \in N^-_i} c_{ji}$ and $c^+_i = \sum_{j \in N^+_i} c_{ij}$ is the aggregate number of transitions arriving in and originating from $i$, respectively.
Note that for every $i$, the weights $\{ w_{ij} \mid j \in N^+_i \}$ are equivalent up to rescaling.

This generalization is relevant in situations where the current context modulates the alternatives' strength.
For example, this could be used to take into account the position or prominence of a link on a page in a hyperlink graph, or the distance between two locations in a mobility network.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Minimal Sufficiency of Marginal Counts}

Recall that $c_{ij}$ denotes the number of times we observe a transition from $i$ to $j$.
We set out to prove the following theorem for the weighted network choice model.

\begin{theorem}
Let $c^-_i = \sum_{j \in N^-_i} c_{ji}$ and $c^+_i = \sum_{j \in N^+_i} c_{ij}$ be the aggregate number of transitions arriving in and originating from $i$, respectively.
Then, $\{ (c^-_i, c^+_i) \mid i \in V \}$ is a minimally sufficient statistic for the parameter $\bm{\lambda}$ in the weighted network choice model.
\end{theorem}

\begin{proof}
Let $f(\{ c_{ij} \} \mid \bm{\lambda})$ be the discrete probability density function of the data under the model with parameters $\bm{\lambda}$.
Theorem $6.2.13$ in \citet{casella2002statistical} states that $\{ (c^-_i, c^+_i) \}$ is a minimally sufficient statistic for $\bm{\lambda}$ if and only if, for any $\{ c_{ij} \}$ and $\{ d_{ij} \}$ in the support of $f$,
\begin{align}
\label{eq:minsuff}
\begin{aligned}
\frac{ f(\{ c_{ij} \} \mid \bm{\lambda}) }{ f(\{ d_{ij} \} \mid \bm{\lambda}) }\ \text{is independent of $\bm{\lambda}$}
\iff (c^-_i, c^+_i) = (d^-_i, d^+_i) \quad \forall i.
\end{aligned}
\end{align}
Taking the log of the ratio on the left-hand side and using~\eqref{eq:wloglik}, we find that
\begin{align*}
\log \frac{ f(\{ c_{ij} \} \mid \bm{\lambda}) }{ f(\{ d_{ij} \} \mid \bm{\lambda}) } =
  \sum_{i = 1}^n \bigg[ (c^-_i\!-\!d^-_i) \log \lambda_i
                       - (c^+_i\!-\!d^+_i) \log\!\sum_{k \in N^+_i}\!w_{ik} \lambda_k \bigg] + \kappa_2.
\end{align*}
From this, it is easy to see that the ratio of densities is independent of $\bm{\lambda}$ if and only if $c^-_i = d^-_i$ and $c^+_i = d^+_i$, which verifies~\eqref{eq:minsuff}.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Well-Posedness of MAP Inference}

Using a $\text{Gamma}(\alpha, \beta)$ prior for each parameter, the log-posterior of the weighted network choice model can be written as
\begin{align}
\label{eq:wlogpost}
\begin{aligned}
&\log p(\bm{\lambda} \mid \mathcal{D}) =
    \sum_{i = 1}^n \bigg[ (c^-_i + \alpha - 1) \log \lambda_i
        - c^+_i \log \bigg( \sum_{k \in N^+_i} w_{ik} \lambda_k \bigg)  - \beta \lambda_i \bigg]
    + \kappa_3.
\end{aligned}
\end{align}
We prove a theorem that guarantees that MAP estimation is well-posed in this generalized model; the proof of Theorem~\ref{thm:map} follows trivially.

\begin{theorem}
\label{thm:wmap}
If i.i.d. $\lambda_1, \ldots, \lambda_n \sim \text{Gamma}(\alpha, \beta)$ with $\alpha > 1$, then there exists a unique maximizer $\bm{\lambda}^\star \in \mathbf{R}^n_{>0}$ of the weighted network choice model's log-posterior~\eqref{eq:wlogpost}.
\end{theorem}

\begin{proof}
The log-posterior~\eqref{eq:wlogpost} is not concave in $\bm{\lambda}$, but it can be made concave using the simple reparametrization $\lambda_i = e^{\theta_i}$.
Under this reparametrization, the log-prior and the log-likelihood become
\begin{align*}
\log p(\bm{\theta})
    &= \sum_{i = 1}^n \left[ (\alpha - 1) \theta_i - \beta e^{\theta_i} \right] + \kappa_4, \\
\ell(\bm{\theta} ; \mathcal{D})
    &= \sum_{i = 1}^n \bigg[ c^-_i \theta_i - c^+_i \log \sum_{k \in N^+_i} w_{ik} e^{\theta_k} \bigg] + \kappa_5.
\end{align*}
It is easy to see that the log-likelihood is concave and the log-prior strictly concave in $\bm{\theta}$.
As a result, the log-posterior is strictly concave in $\bm{\theta}$, which ensures that there exists at most one maximizer.

Now consider any transition counts $\{ c_{ij} \}$ that satisfy $c^-_i = \sum_{j \in N^-_i} c_{ji}$ and $c^+_i = \sum_{j \in N^+_i} c_{ij}$.
The log-posterior can be written as
\begin{align*}
\log p(\bm{\theta} \mid \mathcal{D})
    &= \sum_{i = 1}^n \sum_{j \in N^+_i} c_{ij} \bigg[ \theta_j - \log \sum_{k \in N^+_i} w_{ik} e^{\theta_k} \bigg]
       + \sum_{i = 1}^n \left[ (\alpha - 1) \theta_i - \beta e^{\theta_i} \right] + \kappa_3\\
    &\le -n^2 \cdot \max_{i,j} \log w_{ij}
       + \sum_i^n \left[ (\alpha - 1) \theta_i - \beta e^{\theta_i} \right] + \kappa_3.
\end{align*}
For $\alpha > 1$, it follows that $\lim_{\lVert \bm{\theta} \rVert \to \infty} \log p(\bm{\theta} \mid \mathcal{D}) = -\infty$, which ensures that there is at least one maximizer.
\end{proof}

Note that Theorem~\ref{thm:wmap} can easily be extended to independent but non-identical Gamma priors, where $\lambda_i \sim \text{Gamma}(\alpha_i, \beta_i)$ and $\alpha_i \ne \alpha_j$, $\beta_i \ne \beta_j$ in general.
