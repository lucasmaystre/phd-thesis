\section{Bound on Error Rate of ML Estimate}

We use the analytical framework of \citet{negahban2017rank} to bound the error rate of the ML estimator in the case where
\begin{enuminline}
\item the data is in the form of pairwise comparisons and
\item for each pair under comparison, we observe exactly $k$ outcomes.
\end{enuminline}

Let $G = (V, E)$ be an undirected graph where $V = \{ 1, \ldots, n \}$ and $(i, j) \in E$ if $i$ and $j$ have been compared.
Let $d_{\min}$ and $d_{\max}$ be the minimum and maximum degree of a node in $G$, respectively.
Let $\gamma$ be the spectral gap of a simple random walk on $G$;
intuitively, the larger the spectral gap is, the faster the convergence to the stationary distribution is.
For each $(i, j) \in E$ we observe $k$ comparisons generated from ground truth parameters $\bm{\pi}^*$.
Let $A_{ji}$ denote the number of times $i$ wins against $j$ and $a_{ji} = A_{ji} / k$ the ratio of wins of $i$ over $j$.
We say that an event $X$ occurs with high probability if $\mathbf{P}(X) \ge 1 - c / n^\alpha$ for $c, \alpha$ fixed.

\begin{theorem}
\label{fix:thm:mlbound}
For $k \ge 4C^2 (1 + (b^6 \kappa^2 / (d_{\max} \gamma^2)) \log n)$, the error on the ML estimate $\mlpi$ satisfies w.h.p.
\begin{align}
\frac{\Vert \mlpi - \bm{\pi}^* \Vert_2}{\Vert \bm{\pi}^* \Vert_2} < C \frac{b^{7/2} \kappa}{\gamma} \sqrt{\frac{\log n}{kd_{\max}}},
\end{align}
where $C$ is a constant, $b = \max_{i,j} \pi^*_i / \pi^*_j$ and $\kappa = d_{\max} / d_{\min}$.
\end{theorem}

\begin{proof}
The ML estimate can be interpreted as the stationary distribution of the discrete-time Markov chain
\begin{align}
\label{fix:eq:mlchain}
  \widehat{P}_{ij} =
  \begin{dcases}
    \epsilon \frac{a_{ij}}{\hat{\pi}_i + \hat{\pi}_j}                    & \text{if } i \ne j, \\
    1 - \epsilon \sum_{l \ne i} \frac{a_{il}}{\hat{\pi}_i + \hat{\pi}_l} & \text{if } i = j.
  \end{dcases}
\end{align}
The factor $\epsilon = \hat{\pi}_{\min} / d_{\max}$ ensures that $\widehat{P}$ is stochastic.
Given this matrix, it is straightforward to analyze the $\mlpi$ by using the methods developed for Rank Centrality (RC);
the proof essentially follows that of Theorem~1 of \citet{negahban2017rank}.
Let $P^*$ be the ideal Markov chain, when  $a_{ij} = \pi^*_j / (\pi^*_i + \pi^*_j)$, i.e., the ratios are noiseless.
The key observation is to note that the stationary distribution of $P^*$ is $\bm{\pi}^*$, the true model parameters.
By bounding $\Vert \widehat{P} - P^* \Vert_2$ and $1 - \lambda_{\max}(P^*)$, we can bound the error on the stationary distribution of $P^*$.
For the former, a straightforward application of the proof in the RC case suffices.
For the latter, in the application of the comparison theorem, the lower bound on $\min_{i,j} \pi^*_i P^*_{ij}$ changes by a factor of $1/(2b)$.
This is due to the additional factor $\hat{\pi}_{\min} / (\hat{\pi}_i + \hat{\pi}_j)$ in the off-diagonal entries of $P^*$.
\end{proof}

If the graph of comparisons $G$ is an expander, then $\gamma = O(1)$.
Furthermore, if $d_{\max} \propto d_{\min}$, then $\kappa = O(1)$.
A realization of the $G(n, p)$ random graph satisfies these two constraints with high probability as long as $p = \omega(\log n / n)$.
It follows that if $\omega(n \log n)$ comparison pairs are chosen uniformly at random and $k = O(1)$ outcomes are observed for each pair, the error goes to zero as $n$ increases.

\citet{hajek2014minimax} recently proved a more general version of our result, using a different analytical technique.
Their bound is qualitatively similar, but also applies to multiway rankings and heterogeneous number of comparisons.
