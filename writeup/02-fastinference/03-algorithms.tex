%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithms}
\label{fi:sec:algorithms}

We begin by expressing the MLE under the choice model as the stationary distribution of a Markov chain.
We then take advantage of this formulation to propose novel algorithms for model inference.
Although our derivation is made in the general choice model, we will also discuss implications for the special cases of pairwise data in Section~\ref{fi:sec:pairwise}, $K$-way ranking data in Section~\ref{fi:sec:partial}, and pairwise comparisons with ties in Section~\ref{fi:sec:ties}.

\subsection{MLE as a Stationary Distribution}

For each item $i \in [N]$, we define two sets of indices.
Let $\mathcal{W}_i \doteq \{ m : i \in \mathcal{A}_m, c_m = i \}$ and $\mathcal{L}_i \doteq \{ m : i \in \mathcal{A}_m, c_m \ne i \}$ be the indices of the observations where item $i$ wins over and loses against the alternatives, respectively.
We start from the log-likelihood $\ell(\bm{\gamma})$ in~\eqref{fi:eq:loglik};
the optimality condition $\nabla \ell = 0$ implies
\begin{align}
\left. \frac{\partial \ell(\bm{\gamma})}{\partial \gamma_i} \right\rvert_{\bm{\gamma} = \bm{\gamma}^\star}
    = \sum_{m \in W_i} \bigg[ \frac{1}{\gamma^\star_i} - \frac{1}{\sum_{j \in \mathcal{A}_m} \gamma^\star_j} \bigg]
      - \sum_{m \in \mathcal{L}_i} \frac{1}{\sum_{j \in \mathcal{A}_m} \gamma^\star_j} = 0 \quad \forall i \label{fi:eq:step1} \\
\iff  \sum_{j \ne i} \left[
    \sum_{m \in \mathcal{W}_i \cap \mathcal{L}_j} \frac{\gamma^\star_j}{\sum_{k \in \mathcal{A}_m} \gamma^\star_k}
    \;-\; \sum_{m \in \mathcal{W}_j \cap \mathcal{L}_i} \frac{\gamma^\star_i}{\sum_{k \in \mathcal{A}_m} \gamma^\star_k}
    \right] = 0 \quad \forall i. \label{fi:eq:mlbalance}
\end{align}
In order to go from \eqref{fi:eq:step1} to \eqref{fi:eq:mlbalance}, we multiply by $\gamma^\star_i$ and rearrange the terms.
To simplify the notation, let us further introduce the function
\begin{align*}
f(\mathcal{S}, \bm{\gamma}) \doteq \sum_{\mathcal{A} \in \mathcal{S}} \frac{1}{\sum_{i \in \mathcal{A}} \gamma_i},
\end{align*}
which takes observations $\mathcal{S} \subseteq \mathcal{D}$ and an instance of model parameters $\bm{\gamma}$, and returns a non-negative real number.
Let $\mathcal{D}_{i \succ j} \doteq \{ (c_m, \mathcal{A}_m) \in \mathcal{D} : m \in \mathcal{W}_i \cap \mathcal{L}_j \}$, i.e., the set of observations where $i$ wins over $j$.
Then \eqref{fi:eq:mlbalance} can be rewritten as
\begin{align}
\label{fi:eq:master}
\sum_{j \ne i} \gamma^\star_i \cdot f(\mathcal{D}_{j \succ i}, \bm{\gamma}^\star)
= \sum_{j \ne i} \gamma^\star_j \cdot f(\mathcal{D}_{i \succ j}, \bm{\gamma}^\star) \quad \forall i.
\end{align}
This formulation conveys a new viewpoint on the MLE.
It is easy to recognize the global balance equations \eqref{fi:eq:balance} of a Markov chain on $N$ states (representing the items), with transition rates $\lambda_{ji} = f(\mathcal{D}_{i \succ j}, \bm{\gamma}^\star)$ and stationary distribution $\bm{\gamma}^\star$.
These transition rates have an interesting interpretation: $f(\mathcal{D}_{i \succ j}, \bm{\gamma})$ is the count of how many times $i$ wins over $j$, weighted by the strength of the alternatives.
At this point, it is useful to observe that for any parameters $\bm{\gamma}$, $f(\mathcal{D}_{i \succ j}, \bm{\gamma}) > 0$ if and only if $(j,i) \in \mathcal{E}$.
Combined with the assumption that $\mathcal{G}$ is strongly connected, it follows that any $\bm{\gamma}$ parametrizes the transition rates of an ergodic (homogeneous) Markov chain.
The ergodicity of the inhomogeneous Markov chain, where the transition rates are constantly updated to reflect the current distribution over states, is shown by the following theorem.
\begin{theorem}
\label{fi:thm:convergence}
The Markov chain with inhomogeneous transition rates $\lambda_{ji} = f(\mathcal{D}_{i \succ j}, \bm{\gamma})$ converges to the maximum-likelihood estimate $\bm{\gamma}^\star$, for any initial distribution in the open probability simplex.
\end{theorem}

\begin{proof}[Proof]
Let $\bm{Q}(\bm{\gamma})$ be the infinitesimal generator matrix of the Markov chain $\bm{\gamma}(t)$.
The dynamics of the Markov chain are described by the differential equation
\begin{align}
\label{fi:eq:dynsys}
\frac{d \bm{\gamma}}{dt} = \bm{\gamma}^\tr \bm{Q}(\bm{\gamma}).
\end{align}
By construction, the invariant distributions of the Markov chain coincide with the maximizers of the log-likelihood~\eqref{fi:eq:loglik}.
Hence, we know that $\bm{\gamma}^\star$ is the unique equilibrium point for~\eqref{fi:eq:dynsys}, i.e., satisfying $\bm{\gamma}^\tr \bm{Q}(\bm{\gamma}) = \bm{0}$.
We will now show that this point is globally and asymptotically stable, i.e., $\bm{\gamma}(t) \to \bm{\gamma}^\star$ as $t \to \infty$ for any $\bm{\gamma}(0)$ in the open probability simplex.
To this end, it suffices to show that $V(\bm{\gamma}) = - \ell(\bm{\gamma}) + \ell(\bm{\gamma}^\star)$ is a Lyapunov function for the dynamical system~\eqref{fi:eq:dynsys}.
First, we have that $V(\bm{\gamma}^\star) = 0$ and $V(\bm{\gamma}) > 0$ for all $\bm{\gamma} \ne \bm{\gamma}^\star$ (by definition of the MLE).
Second, we note that $\bm{\gamma}^\tr \bm{Q}(\bm{\gamma}) = \Diag{\bm{\gamma}} \nabla \ell(\bm{\gamma})$.
Hence,
\begin{align*}
\frac{dV}{dt}
    = \left( \nabla V \right)^\tr \frac{d \bm{\gamma}}{dt}
    = - [\nabla \ell(\bm{\gamma})]^\tr \Diag{\bm{\gamma}} \nabla \ell(\bm{\gamma})
    < 0,
\end{align*}
for all $\bm{\gamma} \ne \bm{\gamma}^\star$.
Third, $\ell(\bm{\gamma})$ grows unboundedly as $\bm{\gamma}$ approaches the boundary of the probability simplex \citep[Lemma~1]{hunter2004mm} and therefore $V(\bm{\gamma})$ does so as well.
The result then follows by applying the Barbashin-Krasovskii theorem, a standard result found, e.g., in \citet[Chapter~3]{khalil1996nonlinear}.
\end{proof}


\subsection{Approximate and Exact ML Inference}

We approximate the Markov chain described in \eqref{fi:eq:master} by considering a priori that all alternatives have equal strength.
That is, we set the transition rates $\lambda_{ji} \doteq f(\mathcal{D}_{i \succ j}, \bm{\gamma})$ by fixing $\bm{\gamma}$ to $[1/N \; \cdots \; 1/N]^\tr$.
For $i \ne j$, the contribution of $i$ winning over $j$ to the rate of transition $\lambda_{ji}$ is $N / \Abs{\mathcal{A}}$.
In other words, for each observation, the winning item is rewarded by a fixed amount of incoming rate that is evenly split across the alternatives (the chunk allocated to itself is discarded).
We interpret the stationary distribution $\bar{\bm{\gamma}}$ as an estimate of model parameters.
Algorithm~\ref{fi:alg:lsr} summarizes this procedure, called \emph{Luce Spectral Ranking} (LSR).
%Note that as this Markov chain has different transition rates than that of \eqref{fi:eq:master}, $\bar{\bm{\gamma}} \ne \bm{\gamma}^\star$ in general.
If we consider a growing number of observations, LSR converges to the true model parameters $\bm{\gamma}'$, even in the restrictive case where the sets of alternatives are fixed.

\begin{algorithm}[t]
  \caption{Luce Spectral Ranking}
  \label{fi:alg:lsr}
  \begin{algorithmic}[1]
    \Require observations $\mathcal{D}$
    \State $\bm{\Lambda} \gets \bm{0}_{N \times N}$
    \For{$(i, \mathcal{A}) \in \mathcal{D}$}
      \For{$j \in \mathcal{A} \setminus \{ i \}$}
        \State $\lambda_{ji} \gets \lambda_{ji} + N / \Abs{\mathcal{A}}$
      \EndFor
    \EndFor
    \State \Return stationary distribution of Markov chain $\bm{\Lambda}$
  \end{algorithmic}
\end{algorithm}

\begin{theorem}
\label{fi:thm:consistency}
Let $\mathcal{U} = \{ \mathcal{A}_n \}$ be a collection of sets of alternatives such that for any partition of $\mathcal{U}$ into two non-empty sets $\mathcal{S}$ and $\mathcal{T}$, $\left( \cup_{\mathcal{A} \in \mathcal{S}} \mathcal{A} \right) \cap \left( \cup_{\mathcal{A} \in \mathcal{T}} \mathcal{A} \right) \ne \varnothing$.
Let $M_n$ be the number of choices observed over alternatives $\mathcal{A}_n$.
Then $\bar{\bm{\gamma}} \to \bm{\gamma}'$ as $M_n \to \infty \ \forall n$.
\end{theorem}

\begin{proof}
Let $M \to \infty$ be a shorthand for $M_n \to \infty \ \forall n$.
The condition on $\mathcal{U}$ is equivalent to stating that the hypergraph $H = (\mathcal{V}, \mathcal{U})$, with $\mathcal{V} = [N]$, is connected.
It implies that, asymptotically, the comparison graph $\mathcal{G}$ is strongly connected.
Indeed, for a given set of alternatives $\mathcal{A}_n$, let $i, j \in \mathcal{A}_n$.
The probability that $(j, i) \in \mathcal{E}$ is
\begin{align*}
1 - \left(1 - \frac{\gamma'_i}{\sum_{k \in \mathcal{A}_n} \gamma'_k} \right)^{M_n}
> 1 - (1 - \gamma'_i)^{M_n}
\xrightarrow{M_n \to \infty} 1,
\end{align*}
where we use the fact that $\gamma'_i > 0$ for all $i$.
Therefore, asymptotically, every alternative set $\mathcal{A}_n$ forms a clique in $\mathcal{G}$.
By assumption of connectivity on the hypergraph $\mathcal{H}$, the comparison graph $\mathcal{G}$ is strongly connected.

Now that we know that the Markov chain is asymptotically ergodic, we will show that the stationary distribution matches the true model parameters.
Let $c^n_m$ be a random variable denoting the item chosen in the $m$-th observation over alternatives $\mathcal{A}_n$, and let
$1\{\mathcal{X}\}$ be the indicator variable for the event $\mathcal{X}$.
By the law of large numbers, for any item $i \in \mathcal{A}_n$,
\begin{align}
\label{fi:eq:lln}
\lim_{M_n \to \infty} \frac{1}{M_n} \sum_{m = 1}^{M_n} 1\{c^n_m = i\} = \frac{\gamma'_i}{\sum_{k \in A_n} \gamma'_k}.
\end{align}
Now consider two items $i$ and $j$.
If they have never been compared, $\lambda_{ij} = \lambda_{ji} = 0$.
Otherwise, suppose that they have been compared in alternative sets whose indices are in $\mathcal{B} = \{ n : i, j \in \mathcal{A}_n \}$
By construction of the transition rates in LSR, we have that
\begin{align*}
\frac{\lambda_{ij}}{\lambda_{ji}}
= \frac{\sum_{n \in \mathcal{B}} \sum_{m = 1}^{M_n} 1\{c^n_m = j\} \ N / \Abs{\mathcal{A}_n}}
       {\sum_{n \in \mathcal{B}} \sum_{m = 1}^{M_n} 1\{c^n_m = i\} \ N / \Abs{\mathcal{A}_n}}.
\end{align*}
From \eqref{fi:eq:lln} it follows that
\begin{align*}
\lim_{M \to \infty}\frac{\lambda_{ij}}{\lambda_{ji}}
    = \frac{\sum_{n \in \mathcal{B}} (\gamma'_j / \sum_{k \in A_n} \gamma'_k) \ N / \Abs{\mathcal{A}_n}}
         {\sum_{n \in \mathcal{B}} (\gamma'_i / \sum_{k \in A_n} \gamma'_k) \ N / \Abs{\mathcal{A}_n}}
    = \frac{\gamma'_j }{\gamma'_i}.
\end{align*}
Therefore, when $M \to \infty$,
\begin{align*}
\sum_{j \ne i} \gamma'_i \lambda_{ij} = \sum_{j \ne i} \gamma'_i \left( \frac{\gamma'_j}{\gamma'_i} \lambda_{ji} \right)
                                    = \sum_{j \ne i} \gamma'_j \lambda_{ji}  \quad \forall i.
\end{align*}
It is easy to recognize the global balance equations, and it follows that $\bm{\gamma}'$ is the stationary distribution of the asymptotical Markov chain.
\end{proof}

Starting from the LSR estimate, we can iteratively refine the transition rates of the Markov chain and obtain a sequence of estimates.
By \eqref{fi:eq:master}, the only fixed point of this iteration is the MLE $\bm{\gamma}^\star$.
We call this procedure I-LSR and describe it in Algorithm~\ref{fi:alg:ilsr}.

%In Section~\ref{fi:sec:experimental}, we will investigate in detail the performance and practical advantages of this algorithm over the others for computing the ML estimate.
%For now, we will only note that because LSR is consistent implies that asymptotically, a single iteration is enough.
%This is in contrast to the MM algorithm \citep{hunter2004mm}, which is not consistent.

\begin{algorithm}[t]
  \caption{Iterative Luce Spectral Ranking}
  \label{fi:alg:ilsr}
  \begin{algorithmic}[1]
    \Require observations $\mathcal{D}$
    \State $\bm{\gamma} \gets [1/N \; \cdots \; 1/N]^\tr$
    \Repeat
      \State $\bm{\Lambda} \gets \bm{0}_{N \times N}$
      \For{$(i, \mathcal{A}) \in \mathcal{D}$}
        \For{$j \in \mathcal{A} \setminus \{ i \}$}
          \State $\lambda_{ji} \gets \lambda_{ji} + 1 / \sum_{k \in \mathcal{A}} \gamma_k$
        \EndFor
      \EndFor
      \State $\bm{\gamma} \gets$ stationary distribution of Markov chain $\bm{\Lambda}$
    \Until{convergence}
  \end{algorithmic}
\end{algorithm}

LSR (or one iteration of I-LSR) entails 
\begin{enuminline}
\item filling a matrix of (weighted) pairwise counts and
\item finding a stationary distribution.
\end{enuminline}
Let $D \doteq \sum_{m} \Abs{\mathcal{A}_m}$, and let $S$ be the running time of finding the stationary distribution.
Then LSR has running time $O(D + S)$.
As a comparison, one iteration of the MM algorithm \citep{hunter2004mm} is $O(D)$.
Finding the stationary distribution can be implemented in different ways.
% with $S$ ranging from $O(\min(D, N^2))$ for power methods to $O(N^3)$ for a dense LU decomposition.
For example, in a sparse regime where $D \ll N^2$, the stationary distribution can be found with the power method in a few $O(D)$ sparse matrix multiplications.
In practice, whether $D$ or $S$ turns out to be dominant in the running time is not a foregone conclusion.

\subsection{Bradley--Terry Model}
\label{fi:sec:pairwise}

A widely-used special case of Luce's choice model occurs when all sets of alternatives contain exactly two items, i.e., when the data consists of pairwise comparisons.
This model was proposed by \citet{zermelo1928berechnung}, and later by \citet{bradley1952rank}.
As the stationary distribution is invariant to changes in the time-scale, we can rescale the transition rates and set $\lambda_{ji} \doteq |\mathcal{D}_{i \succ j}|$ when using LSR on pairwise data.
Let $\mathcal{S}$ be the set containing the pairs of items that have been compared at least once.
In the case where each pair $(i, j) \in \mathcal{S}$ has been compared exactly $C$ times, LSR is strictly equivalent to a continuous-time Markov-chain formulation of Rank Centrality \citep{negahban2012iterative}.
In fact, our derivation justifies Rank Centrality as an approximate ML inference algorithm for the Bradley--Terry model.
Furthermore, we provide a principled extension of Rank Centrality to the case where the number of comparisons observed is unbalanced.
Rank Centrality considers transition rates proportional to the \emph{ratio} of wins, whereas \eqref{fi:eq:master} justifies making transition rates proportional to the \emph{count} of wins.

\citet{negahban2012iterative} also provide an upper bound on the error rate of Rank Centrality, which essentially shows that it is minimax-optimal.
Because the two estimators are equivalent in the setting of balanced pairwise comparisons, the bound also applies to LSR.

\subsection{Plackett--Luce Model}
\label{fi:sec:partial}

Another case of interest is when observations do not consist of only a single choice, but of a ranking over the alternatives.
We now suppose $M$ observations consisting of $K$-way rankings, $2 \le K \le N$.
For conciseness, we suppose that $K$ is the same for all observations.
Let one such observation be $i(1) \succ \cdots \succ i(K)$, where $i(r)$ is the item with $r$-th rank.
The Plackett--Luce model (c.f. Section~\ref{in:sec:choice}) posits
\begin{align*}
\Prob{ i(1) \succ \cdots \succ i(K) }
  = \prod_{r = 1}^{K} \frac{\gamma_{i(r)}}{\sum_{s = r}^{K} \gamma_{i(s)}}.
\end{align*}
A ranking can thus be interpreted as a sequence of $K-1$ independent choices:
choose the first item, then choose the second among the remaining alternatives, etc.
With this point of view in mind, LSR and I-LSR can easily accommodate data consisting of $K$-way rankings, by decomposing the $M$ observations into $M' = M (K - 1)$ choices.

\citet{azari2013generalized} provide a class of consistent estimators for the Plackett--Luce model, using the idea of breaking rankings into pairwise comparisons.
Although they explain their algorithms from a generalized-method-of-moments perspective, it is straightforward to reinterpret their estimators as stationary distributions of particular Markov chains.
In fact, for $K = 2$, their algorithm GMM-F is identical to LSR.
When $K > 2$ however, breaking a ranking into $\binom{K}{2}$ pairwise comparisons implicitly makes the (incorrect) assumption that these comparisons are statistically independent.
The Markov chain that LSR builds breaks rankings into pairwise rate contributions, but weights the contributions differently depending on the rank of the winning item.
In Section~\ref{fi:sec:experimental}, we show that this weighting turns out to be crucial.
Our approach yields a significant improvement in statistical efficiency, yet keeps the same attractive computational cost and ease of use.


\subsection{Rao--Kupper Model}
\label{fi:sec:ties}

The link between the MLE and the stationary distribution of a Markov chain seemingly applies to other variants and extensions of Luce's choice model.
As an illustration, we consider the model proposed by \citet{rao1967ties}, which extends the Bradley--Terry model to the case where a comparison between two items can result in a tie.
This model is useful, e.g., for chess, where a significant fraction of comparison outcomes do not result in either a win or a loss.
Letting $\alpha \in [1, \infty)$, the probabilities of $i$ winning over and tying with $j$, respectively, are given by
\begin{align*}
p(i \succ j) &= \frac{\gamma_i}{\gamma_i + \alpha \gamma_j}, \\
p(i \equiv j) &= \frac{\gamma_i \gamma_j(\alpha^2 - 1)}{(\gamma_i + \alpha\gamma_j)(\alpha \gamma_i + \gamma_j)}.
\end{align*}
Informally, the parameter $\alpha$ controls the expected probability of observing a tie in the comparison of two items of equal strength.
We assume that $\alpha$ is fixed, and derive an expression of the MLE $\bm{\gamma}^\star$.
Let $a_{ji}$ be the number of times $i$ wins over $j$, and $t_{ij} = t_{ji}$ be the number of ties between $i$ and $j$.
The log-likelihood can be written as
\begin{align*}
\ell(\bm{\gamma}) &=
  \sum_i \sum_{j \ne i}
  a_{ji} \left[ \log \gamma_i - \log(\gamma_i + \alpha \gamma_j) \right] \\
    &+ \sum_i \sum_{j > i}
    t_{ij} \left[ \log \gamma_i + \log \gamma_j  + \log(\alpha^2 - 1)
     - \log(\gamma_i + \alpha \gamma_j) - \log(\alpha \gamma_i + \gamma_j) \right].
\end{align*}
This function admits a unique MLE $\bm{\gamma}^\star$, and the optimality condition $\nabla \ell = 0$ implies
\begin{align*}
\left. \frac{\partial \ell(\bm{\gamma})}{\partial \gamma_i} \right\rvert_{\bm{\gamma}= \bm{\gamma}^\star}
    = &\sum_{j \ne i} \Bigg[ a_{ji} \bigg( \frac{1}{\gamma^\star_i} - \frac{1}{\gamma^\star_i + \alpha \gamma^\star_j} \bigg)
      - a_{ij} \frac{\alpha}{\alpha \gamma^\star_i + \gamma^\star_j} \\
      & \qquad {} + t_{ij} \bigg( \frac{1}{\gamma^\star_i} - \frac{1}{\gamma^\star_i + \alpha \gamma^\star_j} - \frac{\alpha}{\alpha \gamma^\star_i + \gamma^\star_j} \bigg) \Bigg] = 0 \\
    \iff & \sum_{j \ne i} \Bigg[ a_{ji} \frac{\alpha \gamma^\star_j}{\gamma^\star_i + \alpha \gamma^\star_j}
      - a_{ij} \frac{\alpha \gamma^\star_i}{\alpha \gamma^\star_i + \gamma^\star_j}
      + t_{ij} \frac{\alpha (\gamma^\star_j)^2 - \alpha (\gamma^\star_i)^2}{(\gamma^\star_i + \alpha \gamma^\star_j)(\alpha \gamma^\star_i + \gamma^\star_j)} \Bigg] = 0 \\
    \iff & \sum_{j \ne i} \Bigg[ \frac{a_{ji} + t_{ji}\tfrac{\gamma^\star_j}{\alpha \gamma^\star_i + \gamma^\star_j}}{\gamma^\star_i + \alpha \gamma^\star_j} \gamma^\star_j
      \; - \; \frac{a_{ij} + t_{ij}\tfrac{\gamma^\star_i}{\gamma^\star_i + \alpha \gamma^\star_j}}{\alpha \gamma^\star_i + \gamma^\star_j} \gamma^\star_i \Bigg] = 0.
\end{align*}
Therefore, the MLE can be interpreted as the stationary distribution of a Markov chain with transition rates
\begin{align*}
\lambda_{ij} = \frac{a_{ij} + t_{ij}\tfrac{\gamma^\star_i}{\gamma^\star_i + \alpha \gamma^\star_j}}{\alpha \gamma^\star_i + \gamma^\star_j}.
\end{align*}
Given these transition rates, the extension of Algorithms~\ref{fi:alg:lsr} and~\ref{fi:alg:ilsr} is straightforward.
For example, for LSR, the transition rates simplify to $\lambda_{ij} \propto a_{ij} + t_{ij} (1 + \alpha)^{-1}$.

Beyond the Rao--Kupper model, we believe that our algorithms can be generalized to further models that are based on the choice axiom.
However, this axiom is key, and other choice models (such as Thurstone's \citeyearpar{thurstone1927law}) do not seem to admit the stationary-distribution interpretation we derive here.
